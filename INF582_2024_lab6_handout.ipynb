{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsD-LMKT7XMt"
      },
      "source": [
        "<center>\n",
        "<h2/>\n",
        "<h2>INF582\n",
        "<br>Lab Session 6: Finetuning LLMs</h2> 16 / 02 / 2024<br> Dr. G. Shang<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> Abdoul Rahim ZEBA<br>\n",
        "\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit to Moodle a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>February 23\n",
        ", 2024 08:29 AM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>\n",
        "\n",
        "In this lab you will learn how to use HuggingFace transformers - The most used library by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwN3KCm5Ec6r"
      },
      "source": [
        "## <b>Preparing the environment and installing libraries, models and data</b>\n",
        "\n",
        "In this section, we will setup the environment on Google Colab (first cell), download the pretraind model (second cell) and the finetuning dataset (third cell)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2a5-VNZM6ZMp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6cGqZ9ZB84Cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\Desktop\\INF582\\inf582.lab6\\libs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "Updating files:  25% (1051/4067)\n",
            "Updating files:  26% (1058/4067)\n",
            "Updating files:  27% (1099/4067)\n",
            "Updating files:  28% (1139/4067)\n",
            "Updating files:  29% (1180/4067)\n",
            "Updating files:  30% (1221/4067)\n",
            "Updating files:  31% (1261/4067)\n",
            "Updating files:  32% (1302/4067)\n",
            "Updating files:  33% (1343/4067)\n",
            "Updating files:  34% (1383/4067)\n",
            "Updating files:  35% (1424/4067)\n",
            "Updating files:  36% (1465/4067)\n",
            "Updating files:  37% (1505/4067)\n",
            "Updating files:  38% (1546/4067)\n",
            "Updating files:  39% (1587/4067)\n",
            "Updating files:  40% (1627/4067)\n",
            "Updating files:  41% (1668/4067)\n",
            "Updating files:  42% (1709/4067)\n",
            "Updating files:  43% (1749/4067)\n",
            "Updating files:  44% (1790/4067)\n",
            "Updating files:  45% (1831/4067)\n",
            "Updating files:  46% (1871/4067)\n",
            "Updating files:  47% (1912/4067)\n",
            "Updating files:  48% (1953/4067)\n",
            "Updating files:  49% (1993/4067)\n",
            "Updating files:  49% (2033/4067)\n",
            "Updating files:  50% (2034/4067)\n",
            "Updating files:  51% (2075/4067)\n",
            "Updating files:  52% (2115/4067)\n",
            "Updating files:  53% (2156/4067)\n",
            "Updating files:  54% (2197/4067)\n",
            "Updating files:  55% (2237/4067)\n",
            "Updating files:  56% (2278/4067)\n",
            "Updating files:  57% (2319/4067)\n",
            "Updating files:  58% (2359/4067)\n",
            "Updating files:  59% (2400/4067)\n",
            "Updating files:  60% (2441/4067)\n",
            "Updating files:  61% (2481/4067)\n",
            "Updating files:  62% (2522/4067)\n",
            "Updating files:  63% (2563/4067)\n",
            "Updating files:  64% (2603/4067)\n",
            "Updating files:  65% (2644/4067)\n",
            "Updating files:  66% (2685/4067)\n",
            "Updating files:  67% (2725/4067)\n",
            "Updating files:  68% (2766/4067)\n",
            "Updating files:  69% (2807/4067)\n",
            "Updating files:  70% (2847/4067)\n",
            "Updating files:  71% (2888/4067)\n",
            "Updating files:  72% (2929/4067)\n",
            "Updating files:  73% (2969/4067)\n",
            "Updating files:  74% (3010/4067)\n",
            "Updating files:  75% (3051/4067)\n",
            "Updating files:  75% (3076/4067)\n",
            "Updating files:  76% (3091/4067)\n",
            "Updating files:  77% (3132/4067)\n",
            "Updating files:  78% (3173/4067)\n",
            "Updating files:  79% (3213/4067)\n",
            "Updating files:  80% (3254/4067)\n",
            "Updating files:  81% (3295/4067)\n",
            "Updating files:  82% (3335/4067)\n",
            "Updating files:  83% (3376/4067)\n",
            "Updating files:  84% (3417/4067)\n",
            "Updating files:  85% (3457/4067)\n",
            "Updating files:  86% (3498/4067)\n",
            "Updating files:  87% (3539/4067)\n",
            "Updating files:  88% (3579/4067)\n",
            "Updating files:  89% (3620/4067)\n",
            "Updating files:  90% (3661/4067)\n",
            "Updating files:  91% (3701/4067)\n",
            "Updating files:  92% (3742/4067)\n",
            "Updating files:  93% (3783/4067)\n",
            "Updating files:  94% (3823/4067)\n",
            "Updating files:  95% (3864/4067)\n",
            "Updating files:  96% (3905/4067)\n",
            "Updating files:  96% (3906/4067)\n",
            "Updating files:  97% (3945/4067)\n",
            "Updating files:  98% (3986/4067)\n",
            "Updating files:  99% (4027/4067)\n",
            "Updating files: 100% (4067/4067)\n",
            "Updating files: 100% (4067/4067), done.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to c:\\users\\hp\\appdata\\local\\temp\\pip-req-build-gbw22ats\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit ce4fff0be7f6464d713f7ac3e0bbaafbc6959ae5\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers==4.38.0.dev0) (3.0.12)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.0.dev0)\n",
            "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers==4.38.0.dev0) (1.23.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers==4.38.0.dev0) (20.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers==4.38.0.dev0) (5.3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers==4.38.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers==4.38.0.dev0) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0.dev0)\n",
            "  Downloading tokenizers-0.15.2-cp38-none-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers==4.38.0.dev0)\n",
            "  Downloading safetensors-0.4.2-cp38-none-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers==4.38.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.6.3)\n",
            "Collecting packaging>=20.0 (from transformers==4.38.0.dev0)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.38.0.dev0) (0.4.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.0.dev0) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.0.dev0) (2.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.0.dev0) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.0.dev0) (2023.11.17)\n",
            "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
            "   ---------------------------------------- 330.1/330.1 kB 5.2 MB/s eta 0:00:00\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "   ---------------------------------------- 53.0/53.0 kB 2.9 MB/s eta 0:00:00\n",
            "Downloading safetensors-0.4.2-cp38-none-win_amd64.whl (268 kB)\n",
            "   ---------------------------------------- 268.7/268.7 kB 5.5 MB/s eta 0:00:00\n",
            "Downloading tokenizers-0.15.2-cp38-none-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 2.2/2.2 MB 15.6 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml): started\n",
            "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8505411 sha256=0d00f88a9812076c081e06519ef1ccd123d55bec405672c77676776e1c339186\n",
            "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-iq1rmqn2\\wheels\\05\\0a\\97\\64ae47c27ba95fae2cb5838e7b4b7247a34d4a8ba5f7092de2\n",
            "Successfully built transformers\n",
            "Installing collected packages: safetensors, packaging, huggingface-hub, tokenizers, transformers"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\HP\\AppData\\Local\\Temp\\pip-req-build-gbw22ats'\n",
            "WARNING: Ignoring invalid distribution -umpy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
            "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spyder 4.1.4 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
            "spyder 4.1.4 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
            "sagemaker 2.168.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 7.0.1 which is incompatible.\n",
            "sagemaker 2.168.0 requires PyYAML==6.0, but you have pyyaml 5.3.1 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.3.1\n",
            "    Uninstalling safetensors-0.3.1:\n",
            "      Successfully uninstalled safetensors-0.3.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 20.4\n",
            "    Uninstalling packaging-20.4:\n",
            "      Successfully uninstalled packaging-20.4\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.15.1\n",
            "    Uninstalling huggingface-hub-0.15.1:\n",
            "      Successfully uninstalled huggingface-hub-0.15.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.26.0\n",
            "    Uninstalling transformers-4.26.0:\n",
            "      Successfully uninstalled transformers-4.26.0\n",
            "Successfully installed huggingface-hub-0.20.3 packaging-23.2 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.0.dev0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sagemaker 2.168.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 7.0.1 which is incompatible.\n",
            "sagemaker 2.168.0 requires protobuf<4.0,>=3.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "sagemaker 2.168.0 requires PyYAML==6.0, but you have pyyaml 5.3.1 which is incompatible.\n",
            "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\n",
            "tensorflow 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!mkdir inf582.lab6 && cd inf582.lab6 && mkdir libs\n",
        "%cd inf582.lab6/libs\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install -qqq accelerate datasets evaluate sentencepiece tensorboardX\n",
        "!pip install -qqq bitsandbytes peft loralib einops\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rxHffsPm-EqW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\Desktop\\INF582\\inf582.lab6\\models\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'unzip' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'rm' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'rm' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "!cd .. && mkdir models\n",
        "%cd ../models\n",
        "\n",
        "!wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/yYQjg9XWekttG5j/download/RoBERTa_small_fr_HuggingFace.zip\" -O \"model_huggingface.zip\"\n",
        "!unzip model_huggingface.zip\n",
        "!rm model_huggingface.zip\n",
        "!rm -rf __MACOSX/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HfZ_znATNdya"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\Desktop\\INF582\\inf582.lab6\\data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'unzip' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'rm' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'rm' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\Desktop\\INF582\\inf582.lab6\n"
          ]
        }
      ],
      "source": [
        "!cd .. && mkdir data\n",
        "%cd ../data\n",
        "!wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/EBHqfR776oCE2Nj/download/cls.books.zip\" -O \"cls.books.zip\"\n",
        "!unzip cls.books.zip\n",
        "!rm cls.books.zip\n",
        "!rm -rf __MACOSX/\n",
        "!mkdir cls.books-json\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAR_P343MCKC"
      },
      "source": [
        "# <b>Part 1: Finetuning $RoBERTa_{small}^{fr}$ using HuggingFace's Transfromers</b>\n",
        "In this section of the lab, we will finetune a HuggingFace checkpoint of our $RoBERTa_{small}^{fr}$ on the CLS_Books dataset.\n",
        "\n",
        "The model is based on RoBERTa. It has four transformer's encoder layers and pretrained with the MLM (Masked Language Model) task for 10 epochs using two\n",
        "Nvidia RTX A6000 GPUs on 8M French documents (approximately 39 GB of French raw text) from the dataset mC4. Our dictionary contains 32k tokens obtained by extracting the most 32k frequent French tokens from the dictionary of XLM-R."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLXl0Pudk4Fn"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 1: Compute the number of parameters of the model manually.</b><br>\n",
        "Hints:\n",
        "\n",
        "*   For details about the architecture check [here](https://github.com/hadi-abdine/fairseq/blob/main/fairseq/models/roberta/model.py#L693-L700).\n",
        "*   You can omit the biases and the parameters of normalization layers.\n",
        "*   Don't count the parameters of the language model head.\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "\n",
        "Based on the architecture of the model, we have 4 transformer's encoder layers. Each layer has 3 sub-layers: a multi-head self-attention mechanism, a feed-forward network, and a normalization layer. The size of the hidden layer is 512. If we omit the biases and the parameters of normalization layer and don't count the parameters of the language model head, we have the following number of parameters for each layer:\n",
        "\n",
        "*Multi-head self-attention mechanism:* \n",
        "There are total 8 attention heads, and in each head, there are Q, K, V vectors each with dimension 512 × 512.\n",
        "The total number of parameters for each head is 512 * 512 * 3 * 8 = 786,432.\n",
        "\n",
        "*Feed-forward network:*\n",
        "512 * 2048 * 4 = 4,194,304\n",
        "\n",
        "*Total number of parameters for each layer:*\n",
        " 12,582,912 + 4,194,304 = 16,777,216\n",
        "\n",
        "*Total number of parameters for the model:*\n",
        "16,777,216 * 4 = 67,108,864\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6Hx8wgdjmOy"
      },
      "source": [
        " We will start by downloading the HuggingFace checkpoint and <b>preparing a json format of the CLS_Books dataset</b> (Which is suitable for HuggingFace's checkpoints finetuning using their run_glue script)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3M090L45oPn"
      },
      "source": [
        "## <b>Converting the CLS_Books dataset to json line files</b>\n",
        "\n",
        "In order to use the implemented script in the transformers library, you need to convert your data to json line files (for each split: train, valid and test). For instance, each line inside you file will consist of one and one sample only, contaning the review (accessed by the key <i>sentence1</i> and its label, accessed by the key <i>label</i>. Below you can find an example from <i>valid.json</i> file.\n",
        "\n",
        "Note that these instructions are not valid for all kind of tasks. For other types of tasks (supported in Hugging face) you have to refer to their github for more details.<br>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "<i>\n",
        "{\"sentence1\":\"Seul ouvrage fran\\u00e7ais sur le th\\u00e8me Produits Structur\\u00e9s \\/ fonds \\u00e0 formule, il permet de fa\\u00e7on p\\u00e9dagogique d'appr\\u00e9hender parfaitement les m\\u00e9canismes financiers utilis\\u00e9s. Une r\\u00e9f\\u00e9rence pour ceux qui veulent comprendre les technicit\\u00e9s de base et les raisons de l'engouement des investisseurs sur ces actifs \\u00e0 hauteur de plusieurs milliards d'euros.\",\"label\":\"1\"}<br>\n",
        "{\"sentence1\":\"Livre tr\\u00e8s int\\u00e9ressant !  mais si comme moi vous cherchez des \\\"infos\\\" sur les techniques de sorties et autres \\\"modes d'emploi\\\", afin de vivre par vous m\\u00eame ce genre d'exp\\u00e9rience, c'est pas le bon livre.  \\u00e7a ne lui enl\\u00e8ve d'ailleurd rien \\u00e0 son int\\u00earet.\",\"label\":\"0\"}\n",
        "</i>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HZZFHEHFyv5F"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/cls.books/train.review'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m SPLITS\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m SPLITS:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/cls.books/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.review\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m         reviews \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/cls.books/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39msplit\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cls.books/train.review'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.review', 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "    with open('data/cls.books/'+split+'.label', 'r') as f:\n",
        "        labels = f.readlines()\n",
        "    with open('data/cls.books-json/'+split+'.json', 'w') as f:\n",
        "        #fill the gap here to create train.json, valid.json and test.json\n",
        "        for review, label in zip(reviews, labels):\n",
        "            data = {'sentence1': review, 'label': label}\n",
        "            data = json.dumps(data)\n",
        "            f.write(data+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNPJTU6wsBWP"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: </b><br>\n",
        "Finetune the pretrained model on the dataset CLS-Books to perform binary text\n",
        "classification:\n",
        "\n",
        "*   Use three different seeds.\n",
        "*   For each seed, choose the checkpoint with best validation accuracy.\n",
        "*   Report the average accuracy and its standard deviation on the test set.\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICnN2FvnhTbs"
      },
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$ using the Transformers Library</b>\n",
        "\n",
        "In order to finetune the model using HuggingFace, you need to use the <b>run_glue.py</b> Python script located in the transformers library. For more details, refer to <a href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\" target=\"_blank\">the Huggingface/transformers repository on Github</a>. <br/>\n",
        "\n",
        "Make sure to use the following hyper-parameters: $\\textit{batch size}=8, \\textit{max number of epochs}: 5, \\textit{optimizer}: Adam, \\textit{max learning rate}: 1e-05,  \\textit{warm up ratio}: 0.06, \\textit{learning rate scheduler}: linear$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-BBIykNjH7A"
      },
      "outputs": [],
      "source": [
        "DATA_SET='books'\n",
        "MODEL='RoBERTa_small_fr_huggingface'\n",
        "MAX_SENTENCES= 8 # fill me, batch size.\n",
        "LR=1e-05 #fill me, learning rate\n",
        "MAX_EPOCH=5 #fill me\n",
        "NUM_CLASSES=2 #fill me\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV2Zla33hK_C"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-0b1ba5b19938>, line 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    --model_name_or_path MODEL \\\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "for SEED in range(SEEDS):\n",
        "  SAVE_DIR= 'checkpoints/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\ \n",
        "\n",
        "  )  \n",
        "#fill me\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx8rADrSsus9"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: </b><br>\n",
        "Finetune a random checkpoint of the model on CLS-Books and compare the result with the one from the previous task.\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-0Lly9IIgdz"
      },
      "source": [
        "## <b>Finetuning a non-pretrained $RoBERTa_{small}^{fr}$ using the Transformers Library</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1L8B51zISr0"
      },
      "outputs": [],
      "source": [
        "# create a random model with same configuration as RoBERTa Small:\n",
        "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
        "model = AutoModel.from_config(AutoConfig.from_pretrained('models/RoBERTa_small_fr_HuggingFace/'))\n",
        "tokenizer = AutoTokenizer.from_pretrained('models/RoBERTa_small_fr_HuggingFace/')\n",
        "tokenizer.save_pretrained('models/RoBERTa_small_fr_HuggingFace_random/')\n",
        "model.save_pretrained('models/RoBERTa_small_fr_HuggingFace_random/')\n",
        "\n",
        "for SEED in range(SEEDS):\n",
        "  SAVE_DIR= 'checkpoints/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/random_'+str(SEED)\n",
        "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "\n",
        "      )#fill me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2UMHjatpFvm"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir checkpoints --port 6007"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUCV4V0ONKeJ"
      },
      "source": [
        "# <b>Part 2: Finetuning BLOOM-560m using HuggingFace's Transfromers</b>\n",
        "In this lab, we will fintune [BLOOM-560m](https://huggingface.co/bigscience/bloom-560m) on a question/answer dataset.\n",
        "\n",
        "To reduce the required GPU VRAM for the finetuning, we will use [LoRA](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) and [quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes) techniques.\n",
        "\n",
        "## <b>Preparing the environment and installing libraries:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHHXf0xHUsx9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC-Kv8g8MSuW"
      },
      "source": [
        "## <b>Loading the model and the tokenizer:</b>\n",
        "In this section, we will load the BLOOM model while using the BitsAndBytes library for quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6TaXDnRVKDq"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"bigscience/bloom-560m\"\n",
        "# fill the gap\n",
        "bnb_config = BitsAndBytesConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIvuxW4lVW_E"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        # fill the gap: get the number of trainable parameters: trainable_params\n",
        "        trainable_params += param.numel() if param.requires_grad else 0\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTgKyxhJMeEP"
      },
      "source": [
        "## <b>Configuring LoRA:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duTYSKKYVamH"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H1bBQaSNVsr"
      },
      "source": [
        "## <b>Test the model before finetuning:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRW7HPX6WCmI"
      },
      "outputs": [],
      "source": [
        "# fill the gap, prompt of the format: \"<human>: Comment je peux créer un compte?  \\n <assistant>: \", with an empty response from the assistant\n",
        "prompt = \"<human>:  Comment je peux créer un compte?  \\n <assistant>: \"\n",
        "print(prompt)\n",
        "\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnmKXlqSWPQq"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbhQySqVMo2T"
      },
      "source": [
        "## <b>Loading the question/answer dataset from HuggingFace:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zR54r9AWQ-d"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"OpenLLM-France/Tutoriel\", data_files=\"ecommerce-faq-fr.json\")\n",
        "pd.DataFrame(data[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oSZX9UcNBsu"
      },
      "source": [
        "## <b>Preparing the finetuning data:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQiJpF41WZEc"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: response\"\n",
        "    return f\"<human>: {data_point['question']}  \\n <assistant>: {data_point['response']}\"\n",
        "\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
        "    return tokenized_full_prompt\n",
        "\n",
        "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfbqJ_cNHDa"
      },
      "source": [
        "## <b>Finetuning:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjBMVb6yW_74"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"experiments\"\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=1,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_steps=80,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B01QbSicXknK"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir experiments/runs --port 6008"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxy9b1f4Nqpd"
      },
      "source": [
        "## <b>Test the model after the finetuning:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCYynNlrXDhf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS_lwrJdXr-Y"
      },
      "outputs": [],
      "source": [
        "def generate_response(question: str) -> str:\n",
        "    # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: \" with an empty response\n",
        "    prompt = f\"<human>: {question}?  \\n <assistant>: \"\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    assistant_start = \"<assistant>:\"\n",
        "    response_start = response.find(assistant_start)\n",
        "    return response[response_start + len(assistant_start) :].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYaO6H_hXsvG"
      },
      "outputs": [],
      "source": [
        "prompt = \"Puis-je retourner un produit s'il s'agit d'un article en liquidation ou en vente finale ?\"\n",
        "print('-', prompt,'\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "prompt = \"Que se passe-t-il lorsque je retourne un article en déstockage ?\"\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "prompt = \"Comment puis-je savoir quand je recevrai ma commande ?\"\n",
        "print(generate_response(prompt))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GwN3KCm5Ec6r",
        "PAR_P343MCKC",
        "mUCV4V0ONKeJ",
        "MC-Kv8g8MSuW"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
