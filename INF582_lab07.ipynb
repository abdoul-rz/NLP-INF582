{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labelling\n",
    "\n",
    "In this session we will build an HMM model for PoS-tagging and then CRF and neural models for Named Entity Recognition.\n",
    "\n",
    "## Building a simple Hidden Markov Model\n",
    "\n",
    "In this first part of the lab we will build a very simple bigram HMM using probability estimates over the Brown corpus, which is Part-of-Speech tagged.\n",
    "\n",
    "Recall from course 6: probability estimates (with MLE estimation) can be calculated by dividing the number of occurrences of a bigram by the number of occurrences of the first word.\n",
    "\n",
    "First of all, we import the corpus where we will estimate the probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus is in the form of sequences of sentences, where each sentence is made by a sequence of pairs (word, POS-tag), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall (from the course) that a Hidden Markov Model is composed by:\n",
    "\n",
    "- A set of $N$ states $Q = \\{q_1, q_2, \\ldots, q_N\\}$\n",
    "- A transition probability matrix $A=a_{11}\\ldots a_{ij} \\ldots a_{NN}$, where each $a_{ij}$ represents the probability of transitioning from state $q_i$ to $q_j$; note that $\\sum_{j=1}^N{a_{ij}} = 1 \\forall i$\n",
    "- A sequence of $T$ observations $O = o_1, o_2, \\ldots o_T$, each one drawn from a vocabulary of size $V=v_1, v_2, \\ldots, v_M$, of size $M$;\n",
    "- A sequence of *observation likelihoods* $B=b_i(o_t)$, also called **emission probabilities**, each expressing the probability of an observation $o_t$ being generated from a state $q_i$;\n",
    "- Finally, an initial probability distribution $\\Pi = \\pi_i, \\pi_2, \\ldots, \\pi_N$ where $\\pi_i$ indicates the probability that the Markov chain will start from state $q_i$. Some states $q_j$ may have $\\pi_j = 0$, meaning that they cannot be initial states. Also, $\\sum_{i=1}^N{\\pi_i}=1$.\n",
    "\n",
    "In our case, the set of states $Q$ is made by the vocabulary of labels (the POS-tags). The vocabulary $V$ corresponds to the word vocabulary (i.e. all the set of different words that appear in our corpus). The observations correspond to the sentences in the corpus.\n",
    "\n",
    "We will now split our corpus in a training and test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents()\n",
    "\n",
    "training = corpus[:-10]\n",
    "testing = corpus[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Extract $Q$ and $V$ from the Brown corpus and determine their respective size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states in Q:  472\n",
      "Number of words in V:  56057\n"
     ]
    }
   ],
   "source": [
    "#insert your solution here\n",
    "# set of states Q and the vocabulary V\n",
    "Q = {}\n",
    "V = {}\n",
    "i = 0\n",
    "j = 0\n",
    "for sentence in corpus:\n",
    "    for word, tag in sentence:\n",
    "        if tag not in Q:\n",
    "            Q[tag] = i\n",
    "            i += 1\n",
    "        if word not in V:\n",
    "            V[word] = j\n",
    "            j += 1\n",
    "        \n",
    "print(\"Number of states in Q: \", len(Q))\n",
    "print(\"Number of words in V: \", len(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Create the matrices ($A$, $B$ and $\\Pi$) by using the probabilities estimated on the training set; since we are considering bigrams, the probabilities of the transition matrix will be calculated as $\\frac{count(t_{-1}, t)}{count(t_{-1})}$.\n",
    "\n",
    "*Important*: you will need to add smoothing (for instance Lidstone with $k=0.1$) on $B$ otherwise the output will be $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your solution here\n",
    "import numpy as np\n",
    "\n",
    "# initialize pi matrix, A matrix and B matrix\n",
    "pi = np.zeros(len(Q))\n",
    "A = np.zeros((len(Q), len(Q)))\n",
    "k = 0.1\n",
    "B = np.zeros((len(Q), len(V)))\n",
    "\n",
    "for sentence in training:\n",
    "    for i in range(len(sentence)):\n",
    "        # calculate pi matrix\n",
    "        if i == 0:\n",
    "            pi[Q[sentence[0][1]]] += 1\n",
    "            word, tag = sentence[0]\n",
    "            B[Q[tag]][V[word]] += 1\n",
    "        else:\n",
    "            pi[Q[sentence[i][1]]] += 1\n",
    "\n",
    "            prev_tag = sentence[i-1][1]\n",
    "            curr_tag = sentence[i][1]\n",
    "            A[Q[prev_tag]][Q[curr_tag]] += 1\n",
    "\n",
    "            word, tag = sentence[i]\n",
    "            B[Q[tag]][V[word]] += 1\n",
    "\n",
    "pi = pi / np.sum(pi)\n",
    "A = A / np.sum(A, axis=1)[:, None]\n",
    "B = (B + k) / np.sum(B + k, axis=1)[:, None]    \n",
    "\n",
    "\n",
    "#Expected output:\n",
    "#pi matrix such that pi[i] is the probability of starting in state q_i\n",
    "#A matrix (Q x Q sized) such that A[i][j] is the probability of moving from state q_i to state q_j\n",
    "#B matrix (Q x O sized) such that B[i][j] is the probability of state q_i to emit the word (observation) o_j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a model and we can estimate its performance over the test set.\n",
    "\n",
    "To do this, we need the Viterbi algorithm for the decoding. To help you, an implementation of Viterbi is provided:\n",
    "(note: to use this version you need to assign a numeric id to each word and tag, if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 86, 39, 29, 4, 70, 7, 14, 7, 0, 6, 21, 28, 12, 55, 28, 27, 28, 0, 9, 15, 15]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "params is a triple (pi, A, B) where\n",
    "pi = initial probability distribution over states\n",
    "A = transition probability matrix\n",
    "B = emission probability matrix\n",
    "\n",
    "observations is the sequence of observations (in our case, the observed words)\n",
    "\n",
    "the function returns the optimal sequence of states and its score\n",
    "\"\"\"\n",
    "def viterbi(params, observations):\n",
    "    pi, A, B = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "\n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf') #cases that have not been treated\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "\n",
    "    # base case\n",
    "    alpha[0, :] = pi * B[:,observations[0]]\n",
    "\n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * B[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "\n",
    "    return list(reversed(ss)), np.max(alpha[M-1,:])\n",
    "\n",
    "#Example:\n",
    "\n",
    "#original sentence: you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
    "#sentence as sequence of word indexes:\n",
    "word_index= [953, 1856, 598, 115, 56043, 462, 58, 1069, 109, 30, 3702, 41, 713, 1068, 209, 2434, 58, 6953, 69, 755, 56044, 24]\n",
    "predicted, score = viterbi((pi, A, B), word_index)\n",
    "print(predicted)\n",
    "#predicted will be a sequence of tag indexes:\n",
    "#[12, 55, 86, 39, 29, 4, 70, 7, 14, 7, 0, 6, 21, 28, 12, 55, 28, 27, 28, 0, 9, 14, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The sentence is:  you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PPSS         PPSS\n",
      "      MD*         MD*\n",
      "      QL         QL\n",
      "      RB         RB\n",
      "      VB         VBD\n",
      "      IN         RP\n",
      "      IN         IN\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      VB         VB\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VB         VB\n",
      "      TO         TO\n",
      "      VB         VB\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NP         .\n",
      "      .         .\n",
      "  The sentence is:  Additionally , since you're going to be hors de combat pretty soon with sprue , yaws , Delhi boil , the Granville wilt , liver fluke , bilharziasis , and a host of other complications of the hex you've aroused , you mustn't expect to be lionized socially .\n",
      "   ##TRUE##    ##PRED##\n",
      "      RB         RB\n",
      "      ,         ,\n",
      "      CS         CS\n",
      "      PPSS+BER         PPSS+BER\n",
      "      VBG         VBG\n",
      "      TO         TO\n",
      "      BE         BE\n",
      "      FW-RB         VBN\n",
      "      FW-IN         TO\n",
      "      FW-NN         VB\n",
      "      QL         QL\n",
      "      RB         RB\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      NNS         NP\n",
      "      ,         ,\n",
      "      NP         PPSS\n",
      "      NN         VB\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      NP         VBN-TL\n",
      "      NN         NNS-TL\n",
      "      ,         ,\n",
      "      NN         NN\n",
      "      NN         ''\n",
      "      ,         ,\n",
      "      NN         NP\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AP         AP\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         ``\n",
      "      PPSS+HV         PPSS+HV\n",
      "      VBN         VBN\n",
      "      ,         ,\n",
      "      PPSS         PPSS\n",
      "      MD*         MD*\n",
      "      VB         VB\n",
      "      TO         TO\n",
      "      BE         BE\n",
      "      VBN         VBN\n",
      "      RB         RB\n",
      "      .         .\n",
      "  The sentence is:  My advice , if you live long enough to continue your vocation , is that the next time you're attracted by the exotic , pass it up -- it's nothing but a headache .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VB         VB\n",
      "      JJ         JJ\n",
      "      QLP         QLP\n",
      "      TO         TO\n",
      "      VB         VB\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      BEZ         BEZ\n",
      "      CS         CS\n",
      "      AT         AT\n",
      "      AP         AP\n",
      "      NN         NN\n",
      "      PPSS+BER         PPSS+BER\n",
      "      VBN         VBN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      ,         ,\n",
      "      VB         VB\n",
      "      PPO         PPO\n",
      "      RP         RP\n",
      "      --         --\n",
      "      PPS+BEZ         PPS+BEZ\n",
      "      PN         PN\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      .         .\n",
      "  The sentence is:  As you can count on me to do the same .\n",
      "   ##TRUE##    ##PRED##\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      MD         MD\n",
      "      VB         VB\n",
      "      IN         IN\n",
      "      PPO         PPO\n",
      "      TO         TO\n",
      "      DO         DO\n",
      "      AT         AT\n",
      "      AP         AP\n",
      "      .         .\n",
      "  The sentence is:  Compassionately yours ,\n",
      "   ##TRUE##    ##PRED##\n",
      "      RB         CS\n",
      "      PP$$         PP$$\n",
      "      ,         ,\n",
      "  The sentence is:  S. J. Perelman\n",
      "   ##TRUE##    ##PRED##\n",
      "      NP         NP\n",
      "      NP         NP\n",
      "      NP         NP\n",
      "  The sentence is:  revulsion in the desert\n",
      "   ##TRUE##    ##PRED##\n",
      "      NN-HL         NN\n",
      "      IN-HL         IN\n",
      "      AT-HL         AT\n",
      "      NN-HL         NN\n",
      "  The sentence is:  the doors of the D train slid shut , and as I dropped into a seat and , exhaling , looked up across the aisle , the whole aviary in my head burst into song .\n",
      "   ##TRUE##    ##PRED##\n",
      "      AT         AT\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NP-TL         NN\n",
      "      NN         NN\n",
      "      VBD         VBD\n",
      "      VBN         VBN\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VBD         VBD\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      ,         ,\n",
      "      VBG         NP\n",
      "      ,         ,\n",
      "      VBD         VBD\n",
      "      RP         RP\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NN         NNS\n",
      "      IN         IN\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      VBD         NN\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      .         .\n",
      "  The sentence is:  She was a living doll and no mistake -- the blue-black bang , the wide cheekbones , olive-flushed , that betrayed the Cherokee strain in her Midwestern lineage , and the mouth whose only fault , in the novelist's carping phrase , was that the lower lip was a trifle too voluptuous .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PPS         PPS\n",
      "      BEDZ         BEDZ\n",
      "      AT         AT\n",
      "      VBG         VBG\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      --         --\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NNS         NNS\n",
      "      ,         ,\n",
      "      JJ         NP\n",
      "      ,         ,\n",
      "      WPS         WPS\n",
      "      VBD         VBD\n",
      "      AT         AT\n",
      "      NP         JJ\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      PP$         PP$\n",
      "      JJ-TL         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      WP$         WP$\n",
      "      AP         AP\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN$         NN$\n",
      "      VBG         NN\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      BEDZ         BEDZ\n",
      "      CS         CS\n",
      "      AT         AT\n",
      "      JJR         JJR\n",
      "      NN         NN\n",
      "      BEDZ         BEDZ\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      QL         QL\n",
      "      JJ         JJ\n",
      "      .         .\n",
      "  The sentence is:  From what I was able to gauge in a swift , greedy glance , the figure inside the coral-colored boucle dress was stupefying .\n",
      "   ##TRUE##    ##PRED##\n",
      "      IN         IN\n",
      "      WDT         WDT\n",
      "      PPSS         PPSS\n",
      "      BEDZ         BEDZ\n",
      "      JJ         JJ\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      ,         ,\n",
      "      JJ         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         QL\n",
      "      NN         JJ\n",
      "      NN         NN\n",
      "      BEDZ         BEDZ\n",
      "      VBG         VBN\n",
      "      .         .\n"
     ]
    }
   ],
   "source": [
    "#Example of results calculation\n",
    "word_to_index = V\n",
    "Q_ = list(Q.keys())\n",
    "testing_formated = []\n",
    "for sentence in testing:\n",
    "    sent = \"\"\n",
    "    words_index = [] #vector of word indices to be passed to Viterbi\n",
    "    true_label= [] #vector of the true labels from labeled corpus\n",
    "    for word,tag in sentence:\n",
    "        words_index.append(word_to_index[word]) #word_to_index is a dictionary mapping a word to its index\n",
    "        true_label.append(tag)\n",
    "        sent=sent+\" \"+word\n",
    "    testing_formated.append((words_index,true_label,sent))\n",
    "\n",
    "\n",
    "for word_index,labels,sentence in testing_formated:\n",
    "    print(\"  The sentence is:\",sentence)\n",
    "    print(\"   ##TRUE##    ##PRED##\")\n",
    "    predicted, score = viterbi((pi, A, B), word_index) #call the viterbi decoder\n",
    "    for i,true_label in enumerate(labels):\n",
    "        predicted_label = Q_[predicted[i]] #Q here is the vector of tags, so that at Q[i] we have the i_th tag in literal form\n",
    "        print(\"      \"+true_label+\"         \"+predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: calculate Precision, Recall and F-measure for the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8617028156254097\n",
      "Recall:  0.8744769874476988\n",
      "F1-score:  0.8632846592775625\n"
     ]
    }
   ],
   "source": [
    "# Precison, Recall and F1-score for the bigram model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for word_index,labels,sentence in testing_formated:\n",
    "    predicted, score = viterbi((pi, A, B), word_index)\n",
    "    for i,true_label in enumerate(labels):\n",
    "        predicted_label = Q_[predicted[i]]\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(predicted_label)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: modify your HMM to use trigrams instead of bigrams, and re-evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK's HMM implementation\n",
    "\n",
    "We will compare now our model built from scratch to the implementation provided by NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('you', 'PPSS'), (\"can't\", 'MD*'), ('very', 'QL'), ('well', 'RB'), ('sidle', 'VB'), ('up', 'IN'), ('to', 'IN'), ('people', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('street', 'NN'), ('and', 'CC'), ('ask', 'VB'), ('if', 'CS'), ('they', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('buy', 'VB'), ('a', 'AT'), ('hot', 'JJ'), ('Bodhisattva', 'NP'), ('.', '.')]\n",
      "[('you', 'PPSS'), (\"can't\", 'MD*'), ('very', 'QL'), ('well', 'RB'), ('sidle', 'VBD'), ('up', 'RP'), ('to', 'IN'), ('people', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('street', 'NN'), ('and', 'CC'), ('ask', 'VB'), ('if', 'CS'), ('they', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('buy', 'VB'), ('a', 'AT'), ('hot', 'JJ'), ('Bodhisattva', '.'), ('.', '.')]\n",
      "[('Additionally', 'RB'), (',', ','), ('since', 'CS'), (\"you're\", 'PPSS+BER'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('hors', 'FW-RB'), ('de', 'FW-IN'), ('combat', 'FW-NN'), ('pretty', 'QL'), ('soon', 'RB'), ('with', 'IN'), ('sprue', 'NN'), (',', ','), ('yaws', 'NNS'), (',', ','), ('Delhi', 'NP'), ('boil', 'NN'), (',', ','), ('the', 'AT'), ('Granville', 'NP'), ('wilt', 'NN'), (',', ','), ('liver', 'NN'), ('fluke', 'NN'), (',', ','), ('bilharziasis', 'NN'), (',', ','), ('and', 'CC'), ('a', 'AT'), ('host', 'NN'), ('of', 'IN'), ('other', 'AP'), ('complications', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('hex', 'NN'), (\"you've\", 'PPSS+HV'), ('aroused', 'VBN'), (',', ','), ('you', 'PPSS'), (\"mustn't\", 'MD*'), ('expect', 'VB'), ('to', 'TO'), ('be', 'BE'), ('lionized', 'VBN'), ('socially', 'RB'), ('.', '.')]\n",
      "[('Additionally', 'RB'), (',', ','), ('since', 'CS'), (\"you're\", 'PPSS+BER'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('hors', 'VBN'), ('de', 'TO'), ('combat', 'VB'), ('pretty', 'QL'), ('soon', 'RB'), ('with', 'IN'), ('sprue', 'NN'), (',', ','), ('yaws', 'NP'), (',', ','), ('Delhi', 'PPSS'), ('boil', 'VB'), (',', ','), ('the', 'AT'), ('Granville', 'VBN-TL'), ('wilt', 'NNS-TL'), (',', ','), ('liver', 'NN'), ('fluke', \"''\"), (',', ','), ('bilharziasis', 'NP'), (',', ','), ('and', 'CC'), ('a', 'AT'), ('host', 'NN'), ('of', 'IN'), ('other', 'AP'), ('complications', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('hex', '``'), (\"you've\", 'PPSS+HV'), ('aroused', 'VBN'), (',', ','), ('you', 'PPSS'), (\"mustn't\", 'MD*'), ('expect', 'VB'), ('to', 'TO'), ('be', 'BE'), ('lionized', 'VBN'), ('socially', 'RB'), ('.', '.')]\n",
      "[('My', 'PP$'), ('advice', 'NN'), (',', ','), ('if', 'CS'), ('you', 'PPSS'), ('live', 'VB'), ('long', 'JJ'), ('enough', 'QLP'), ('to', 'TO'), ('continue', 'VB'), ('your', 'PP$'), ('vocation', 'NN'), (',', ','), ('is', 'BEZ'), ('that', 'CS'), ('the', 'AT'), ('next', 'AP'), ('time', 'NN'), (\"you're\", 'PPSS+BER'), ('attracted', 'VBN'), ('by', 'IN'), ('the', 'AT'), ('exotic', 'JJ'), (',', ','), ('pass', 'VB'), ('it', 'PPO'), ('up', 'RP'), ('--', '--'), (\"it's\", 'PPS+BEZ'), ('nothing', 'PN'), ('but', 'CC'), ('a', 'AT'), ('headache', 'NN'), ('.', '.')]\n",
      "[('My', 'PP$'), ('advice', 'NN'), (',', ','), ('if', 'CS'), ('you', 'PPSS'), ('live', 'VB'), ('long', 'JJ'), ('enough', 'QLP'), ('to', 'TO'), ('continue', 'VB'), ('your', 'PP$'), ('vocation', 'NN'), (',', ','), ('is', 'BEZ'), ('that', 'CS'), ('the', 'AT'), ('next', 'AP'), ('time', 'NN'), (\"you're\", 'PPSS+BER'), ('attracted', 'VBN'), ('by', 'IN'), ('the', 'AT'), ('exotic', 'JJ'), (',', ','), ('pass', 'VB'), ('it', 'PPO'), ('up', 'RP'), ('--', '--'), (\"it's\", 'PPS+BEZ'), ('nothing', 'PN'), ('but', 'CC'), ('a', 'AT'), ('headache', 'NN'), ('.', '.')]\n",
      "[('As', 'CS'), ('you', 'PPSS'), ('can', 'MD'), ('count', 'VB'), ('on', 'IN'), ('me', 'PPO'), ('to', 'TO'), ('do', 'DO'), ('the', 'AT'), ('same', 'AP'), ('.', '.')]\n",
      "[('As', 'CS'), ('you', 'PPSS'), ('can', 'MD'), ('count', 'VB'), ('on', 'IN'), ('me', 'PPO'), ('to', 'TO'), ('do', 'DO'), ('the', 'AT'), ('same', 'AP'), ('.', '.')]\n",
      "[('Compassionately', 'RB'), ('yours', 'PP$$'), (',', ',')]\n",
      "[('Compassionately', '``'), ('yours', 'UH'), (',', ',')]\n",
      "[('S.', 'NP'), ('J.', 'NP'), ('Perelman', 'NP')]\n",
      "[('S.', 'NP'), ('J.', 'NP'), ('Perelman', 'NP')]\n",
      "[('revulsion', 'NN-HL'), ('in', 'IN-HL'), ('the', 'AT-HL'), ('desert', 'NN-HL')]\n",
      "[('revulsion', 'NN'), ('in', 'IN'), ('the', 'AT'), ('desert', 'NN')]\n",
      "[('the', 'AT'), ('doors', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('D', 'NP-TL'), ('train', 'NN'), ('slid', 'VBD'), ('shut', 'VBN'), (',', ','), ('and', 'CC'), ('as', 'CS'), ('I', 'PPSS'), ('dropped', 'VBD'), ('into', 'IN'), ('a', 'AT'), ('seat', 'NN'), ('and', 'CC'), (',', ','), ('exhaling', 'VBG'), (',', ','), ('looked', 'VBD'), ('up', 'RP'), ('across', 'IN'), ('the', 'AT'), ('aisle', 'NN'), (',', ','), ('the', 'AT'), ('whole', 'JJ'), ('aviary', 'NN'), ('in', 'IN'), ('my', 'PP$'), ('head', 'NN'), ('burst', 'VBD'), ('into', 'IN'), ('song', 'NN'), ('.', '.')]\n",
      "[('the', 'AT'), ('doors', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('D', 'NN'), ('train', 'NN'), ('slid', 'VBD'), ('shut', 'VBN'), (',', ','), ('and', 'CC'), ('as', 'CS'), ('I', 'PPSS'), ('dropped', 'VBD'), ('into', 'IN'), ('a', 'AT'), ('seat', 'NN'), ('and', 'CC'), (',', ','), ('exhaling', 'NP'), (',', ','), ('looked', 'VBD'), ('up', 'RP'), ('across', 'IN'), ('the', 'AT'), ('aisle', 'NN'), (',', ','), ('the', 'AT'), ('whole', 'JJ'), ('aviary', 'NNS'), ('in', 'IN'), ('my', 'PP$'), ('head', 'NN'), ('burst', 'NN'), ('into', 'IN'), ('song', 'NN'), ('.', '.')]\n",
      "[('She', 'PPS'), ('was', 'BEDZ'), ('a', 'AT'), ('living', 'VBG'), ('doll', 'NN'), ('and', 'CC'), ('no', 'AT'), ('mistake', 'NN'), ('--', '--'), ('the', 'AT'), ('blue-black', 'JJ'), ('bang', 'NN'), (',', ','), ('the', 'AT'), ('wide', 'JJ'), ('cheekbones', 'NNS'), (',', ','), ('olive-flushed', 'JJ'), (',', ','), ('that', 'WPS'), ('betrayed', 'VBD'), ('the', 'AT'), ('Cherokee', 'NP'), ('strain', 'NN'), ('in', 'IN'), ('her', 'PP$'), ('Midwestern', 'JJ-TL'), ('lineage', 'NN'), (',', ','), ('and', 'CC'), ('the', 'AT'), ('mouth', 'NN'), ('whose', 'WP$'), ('only', 'AP'), ('fault', 'NN'), (',', ','), ('in', 'IN'), ('the', 'AT'), (\"novelist's\", 'NN$'), ('carping', 'VBG'), ('phrase', 'NN'), (',', ','), ('was', 'BEDZ'), ('that', 'CS'), ('the', 'AT'), ('lower', 'JJR'), ('lip', 'NN'), ('was', 'BEDZ'), ('a', 'AT'), ('trifle', 'NN'), ('too', 'QL'), ('voluptuous', 'JJ'), ('.', '.')]\n",
      "[('She', 'PPS'), ('was', 'BEDZ'), ('a', 'AT'), ('living', 'VBG'), ('doll', 'NN'), ('and', 'CC'), ('no', 'AT'), ('mistake', 'NN'), ('--', '--'), ('the', 'AT'), ('blue-black', 'JJ'), ('bang', 'NN'), (',', ','), ('the', 'AT'), ('wide', 'JJ'), ('cheekbones', 'NNS'), (',', ','), ('olive-flushed', 'NP'), (',', ','), ('that', 'WPS'), ('betrayed', 'VBD'), ('the', 'AT'), ('Cherokee', 'JJ'), ('strain', 'NN'), ('in', 'IN'), ('her', 'PP$'), ('Midwestern', 'JJ'), ('lineage', 'NN'), (',', ','), ('and', 'CC'), ('the', 'AT'), ('mouth', 'NN'), ('whose', 'WP$'), ('only', 'AP'), ('fault', 'NN'), (',', ','), ('in', 'IN'), ('the', 'AT'), (\"novelist's\", 'NN$'), ('carping', 'NN'), ('phrase', 'NN'), (',', ','), ('was', 'BEDZ'), ('that', 'CS'), ('the', 'AT'), ('lower', 'JJR'), ('lip', 'NN'), ('was', 'BEDZ'), ('a', 'AT'), ('trifle', 'NN'), ('too', 'QL'), ('voluptuous', 'JJ'), ('.', '.')]\n",
      "[('From', 'IN'), ('what', 'WDT'), ('I', 'PPSS'), ('was', 'BEDZ'), ('able', 'JJ'), ('to', 'IN'), ('gauge', 'NN'), ('in', 'IN'), ('a', 'AT'), ('swift', 'JJ'), (',', ','), ('greedy', 'JJ'), ('glance', 'NN'), (',', ','), ('the', 'AT'), ('figure', 'NN'), ('inside', 'IN'), ('the', 'AT'), ('coral-colored', 'JJ'), ('boucle', 'NN'), ('dress', 'NN'), ('was', 'BEDZ'), ('stupefying', 'VBG'), ('.', '.')]\n",
      "[('From', 'IN'), ('what', 'WDT'), ('I', 'PPSS'), ('was', 'BEDZ'), ('able', 'JJ'), ('to', 'IN'), ('gauge', 'NN'), ('in', 'IN'), ('a', 'AT'), ('swift', 'JJ'), (',', ','), ('greedy', 'JJ'), ('glance', 'NN'), (',', ','), ('the', 'AT'), ('figure', 'NN'), ('inside', 'IN'), ('the', 'AT'), ('coral-colored', 'QL'), ('boucle', 'JJ'), ('dress', 'NN'), ('was', 'BEDZ'), ('stupefying', 'VBN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer(states = Q, symbols = V)\n",
    "\n",
    "model = trainer.train_supervised(training, estimator=lambda fd, bins: hmm.LidstoneProbDist(fd, 0.1, bins))\n",
    "\n",
    "for sent in testing:\n",
    "    u_sent=[]\n",
    "    for word, tag in sent:\n",
    "        u_sent.append(word)\n",
    "    tagged=model.tag(u_sent)\n",
    "    print(sent)\n",
    "    print(tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Calculate precision, recall and F-measure and compare them to the results that you obtained with the two models (bigram and trigram) that you implemented before. Can you deduce whether the NLTK model is using bigrams or trigrams? (It is not stated in the manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8611798030731084\n",
      "Recall:  0.8702928870292888\n",
      "F1-score:  0.8610531390544105\n"
     ]
    }
   ],
   "source": [
    "# Precison, Recall and F1-score for model trained with nltk\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for sent in testing:\n",
    "    u_sent=[]\n",
    "    for word, tag in sent:\n",
    "        u_sent.append(word)\n",
    "    tagged=model.tag(u_sent)\n",
    "    for i, (word, tag) in enumerate(sent):\n",
    "        y_true.append(tag)\n",
    "        y_pred.append(tagged[i][1])\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-score: \", f1_score)\n",
    "\n",
    "# The model trained with nltk has a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Conditional Random Fields\n",
    "\n",
    "For this exercise we will need to use the sklearn_crfsuite package. If it is not installed, it can be installed using pip with ```pip install sklearn-crfsuite```.\n",
    "\n",
    "We will work on a Kaggle dataset named ```ner_dataset.csv``` (it should be in the same directory as the notebook).\n",
    "\n",
    "Pandas can be used to read the content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Firdaus', 'Reggio', 'TSZ', 'doctrine', 'credit-card', 'Zinedine', '46,000', 'summed', 'U.S.-allied', 'misquoting']\n",
      "35178\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\") #repeat sentence number on each row\n",
    "\n",
    "words = list(set(data[\"Word\"].values)) #vocabulary V\n",
    "n_words = len(words)\n",
    "\n",
    "print(words[:10])\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you with some code that can read the sentences and produce the features in the format required by crf_suite. The ```SentenceGetter``` class transforms sentences into sequences of ```(word, POS, tag)``` triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "#load data\n",
    "getter = SentenceGetter(data) #transform sentences into sequences of (Word, POS, Tag)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function allows us to define features that are used in the CRF. The features are stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    input:\n",
    "       sent: sentence in the format of sequence of (Word, POS, Tag) triples\n",
    "       i: position in the sentence\n",
    "    output:\n",
    "       features: a dictionary mapping the feature name into a value\n",
    "    \"\"\"\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = { #features related to the current position\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "    if i > 0: #features related to preceding word/tag\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True #feature for Beginning of Sentence\n",
    "\n",
    "    if i < len(sent)-1: #features related to the following word/tag\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True #feature for end of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    #transforms the sentence in a sequence of features\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    #transforms the sentence in a sequence of labels\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    #transforms the sentence in a sequence of tokens (removes POS tags and labels)\n",
    "    return [token for token, postag, label in sent]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the features and label vectors, and create a CRF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF(algorithm='lbfgs',  max_iterations=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a model with gradient descent algorithm (\"lbfgs\") and a limit of $100$ iterations.\n",
    "\n",
    "Now we build the model and evaluate it on a 66/33 split between training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.40      0.01      0.03       137\n",
      "       B-eve       0.63      0.30      0.40       111\n",
      "       B-geo       0.84      0.90      0.87     12357\n",
      "       B-gpe       0.98      0.83      0.90      5226\n",
      "       B-nat       0.36      0.25      0.29        69\n",
      "       B-org       0.77      0.69      0.73      6762\n",
      "       B-per       0.83      0.79      0.81      5649\n",
      "       B-tim       0.93      0.84      0.88      6650\n",
      "       I-art       0.38      0.02      0.05       124\n",
      "       I-eve       0.48      0.18      0.26        89\n",
      "       I-geo       0.81      0.78      0.79      2433\n",
      "       I-gpe       0.95      0.33      0.49        55\n",
      "       I-nat       0.25      0.10      0.14        21\n",
      "       I-org       0.74      0.80      0.76      5545\n",
      "       I-per       0.84      0.89      0.86      5730\n",
      "       I-tim       0.85      0.71      0.78      2110\n",
      "           O       0.99      0.99      0.99    292571\n",
      "\n",
      "    accuracy                           0.97    345639\n",
      "   macro avg       0.71      0.55      0.59    345639\n",
      "weighted avg       0.97      0.97      0.97    345639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "crf.fit(X_train, y_train)\n",
    "pred=crf.predict(X_test)\n",
    "n_pred = [item for sublist in pred for item in sublist]\n",
    "n_test = [item for sublist in y_test for item in sublist]\n",
    "report = classification_report(y_pred=n_pred, y_true=n_test)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report shows accuracy stats for all classes, but we are not interested in the **O** class. We can see that the scores for people names, place names and organizations are relatively low.\n",
    "\n",
    "**Exercise 6**: Can you think of some new features for the CRF model to improve the results, especially on **B-org** ? Modify the *word2features* function to include the additional features and compare with the above results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the word2features function to include the word itself and other features\n",
    "# There is improvement on B-org but there are some performance drops on other classes\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.24      0.03      0.05       137\n",
      "       B-eve       0.64      0.34      0.45       111\n",
      "       B-geo       0.85      0.91      0.88     12357\n",
      "       B-gpe       0.96      0.93      0.94      5226\n",
      "       B-nat       0.60      0.13      0.21        69\n",
      "       B-org       0.80      0.71      0.75      6762\n",
      "       B-per       0.83      0.80      0.82      5649\n",
      "       B-tim       0.92      0.87      0.89      6650\n",
      "       I-art       0.23      0.04      0.07       124\n",
      "       I-eve       0.53      0.22      0.31        89\n",
      "       I-geo       0.82      0.81      0.81      2433\n",
      "       I-gpe       0.93      0.49      0.64        55\n",
      "       I-nat       0.00      0.00      0.00        21\n",
      "       I-org       0.78      0.80      0.79      5545\n",
      "       I-per       0.84      0.90      0.87      5730\n",
      "       I-tim       0.82      0.76      0.79      2110\n",
      "           O       0.99      0.99      0.99    292571\n",
      "\n",
      "    accuracy                           0.97    345639\n",
      "   macro avg       0.69      0.57      0.60    345639\n",
      "weighted avg       0.97      0.97      0.97    345639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF(algorithm='lbfgs',  max_iterations=100)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "crf.fit(X_train, y_train)\n",
    "pred=crf.predict(X_test)\n",
    "n_pred = [item for sublist in pred for item in sublist]\n",
    "n_test = [item for sublist in y_test for item in sublist]\n",
    "report = classification_report(y_pred=n_pred, y_true=n_test)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER using a LSTM model\n",
    "\n",
    "In this final section we will see an example of a neural network model written in PyTorch that uses a LSTM-based architecture for Named Entity Recognition.\n",
    "\n",
    "First of all, we will prepare the data to have all information coded numerically (words and tags) and the sentences padded to a max length, in order to have all sentences of the same size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m vocab \u001b[38;5;241m=\u001b[39m {w: i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words)} \u001b[38;5;66;03m#map words into a number\u001b[39;00m\n\u001b[0;32m      6\u001b[0m tag_map \u001b[38;5;241m=\u001b[39m {t: i \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tags)} \u001b[38;5;66;03m#map tags into a number\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      9\u001b[0m max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m75\u001b[39m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m [[vocab[w[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences]\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\distribute\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras' Distribution Strategy library.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sidecar_evaluator\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Python module for evaluation loop.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_handle_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[0;32m     20\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     21\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     22\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m   ,\n\u001b[0;32m     27\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36\u001b[0m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m     18\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     19\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m   serialized_pb\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x64\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x0b\u001b[39;00m\u001b[38;5;130;01m\\x32\u001b[39;00m\u001b[38;5;124m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x14\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;124munknown_rank\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x08\u001b[39;00m\u001b[38;5;130;01m\\x1a\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x44\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124msize\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124mname\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;130;01m\\x87\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;124morg.tensorflow.frameworkB\u001b[39m\u001b[38;5;130;01m\\x11\u001b[39;00m\u001b[38;5;124mTensorShapeProtosP\u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[38;5;130;01m\\xf8\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x62\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     30\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     33\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[0;32m     34\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m   fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 36\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFieldDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_default_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessage_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontaining_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     43\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mFieldDescriptor(\n\u001b[0;32m     44\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     45\u001b[0m       number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, cpp_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     46\u001b[0m       has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, default_value\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     47\u001b[0m       message_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, enum_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m       is_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extension_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, file\u001b[38;5;241m=\u001b[39mDESCRIPTOR),\n\u001b[0;32m     50\u001b[0m   ],\n\u001b[0;32m     51\u001b[0m   extensions\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     52\u001b[0m   ],\n\u001b[0;32m     53\u001b[0m   nested_types\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     54\u001b[0m   enum_types\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     55\u001b[0m   ],\n\u001b[0;32m     56\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m   is_extendable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m   syntax\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     59\u001b[0m   extension_ranges\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     60\u001b[0m   oneofs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     61\u001b[0m   ],\n\u001b[0;32m     62\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m149\u001b[39m,\n\u001b[0;32m     63\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     67\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     68\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    103\u001b[0m _TENSORSHAPEPROTO_DIM\u001b[38;5;241m.\u001b[39mcontaining_type \u001b[38;5;241m=\u001b[39m _TENSORSHAPEPROTO\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\google\\protobuf\\descriptor.py:553\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, full_name, index, number, \u001b[38;5;28mtype\u001b[39m, cpp_type, label,\n\u001b[0;32m    548\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[0;32m    549\u001b[0m             is_extension, extension_scope, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    551\u001b[0m             has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, containing_oneof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    552\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_extension:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _message\u001b[38;5;241m.\u001b[39mdefault_pool\u001b[38;5;241m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "#first of all we need to retrieve the number of different tags (a.k.a categories or classes of the words)\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags=len(tags)\n",
    "\n",
    "vocab = {w: i + 1 for i, w in enumerate(words)} #map words into a number\n",
    "tag_map = {t: i for i, t in enumerate(tags)} #map tags into a number\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len=75\n",
    "\n",
    "X = [[vocab[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words) #pad with special token PAD, with ID=n_words\n",
    "y = [[tag_map[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=-1) # -1 is associated to the PAD token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the data and split them into training and test. We set batch size at 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from NERDataset import NERDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "ner_train = NERDataset(X_train, y_train)\n",
    "ner_test = NERDataset(X_test, y_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(ner_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model is defined here. We have an embedding that maps each word in a vector (embedding) of size 100, which is learnt from the dataset. The embedded sentence is fed to a LSTM layer of size 50. The output is transferred to a fully connected layer with *n_tags* output, one for each of the possible labels. The loss is a cross-entropy loss over all tokens (excluding the \"pad\" tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vocab_size=n_words+1\n",
    "embedding_dim=100\n",
    "lstm_hidden_dim=50\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        #maps each token to an embedding_dim vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #the LSTM takens embedded sentence\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        #fc layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, n_tags)\n",
    "    \n",
    "    def forward(self, s):\n",
    "        #apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
    "        \n",
    "        #run the LSTM along the sentences of length batch_max_len. We discard the cell state as output\n",
    "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim                \n",
    "        \n",
    "        #reshape the Variable so that each row contains one token\n",
    "        s = s.reshape(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
    "        \n",
    "        #apply the fully connected layer and obtain the output for each token\n",
    "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
    "        \n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*batch_max_len x num_tags\n",
    "    \n",
    "def loss_fn(outputs, labels):\n",
    "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)  \n",
    "    \n",
    "    #discard 'PAD' tokens\n",
    "    mask = (labels >= 0).float()\n",
    "    \n",
    "    #the number of tokens is the sum of elements in mask\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "    \n",
    "    #pick the values corresponding to labels and multiply by mask\n",
    "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "    #cross entropy loss for all non 'PAD' tokens\n",
    "    return -torch.sum(outputs)/num_tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block we carry out the training over 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs=5\n",
    "#Run the training loop for defined number of epochs\n",
    "for epoch in range(0, num_epochs):\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        #print(inputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        outputs = network(inputs) # Perform forward pass\n",
    "        loss = loss_fn(outputs, targets) # Compute loss\n",
    "        loss.backward() # Backprop\n",
    "        optimizer.step() # Optimization\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**: Evaluate the model over the test set (see the data preparation cells), compare the result with the previously seen CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# evaluate the model over the test set\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate the model over the test set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(ner_test, batch_size=32, shuffle=True)\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            targets = np.array(targets.view(-1).tolist())\n",
    "            positive_indices = np.where(targets >= 0)\n",
    "            predicted = np.array(predicted.view(-1).tolist())\n",
    "            # Append predictions and true labels to the lists\n",
    "            y_true.extend(targets[positive_indices].tolist())\n",
    "            y_pred.extend(predicted[positive_indices].tolist())\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "y_true, y_pred = evaluate_model(network, test_loader)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=tags))\n",
    "\n",
    "# The performance are worst than the lastest models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**: Modify the model to use GloVe vectors as initial embeddings (hint: see the documentation of the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\">Embedding</a> layer in PyTorch, in particular the *from_pretrained* method), and compare the results to the base one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = GloVe(name='6B', dim=embedding_dim)\n",
    "\n",
    "text_field = Field(lower=True, include_lengths=True, batch_first=True)\n",
    "label_field = LabelField()\n",
    "\n",
    "examples = list(vocab.keys()) \n",
    "\n",
    "text_field.build_vocab(examples, vectors=glove)\n",
    "label_field.build_vocab(examples)\n",
    "\n",
    "\n",
    "Network = Net()\n",
    "Network.embedding = nn.Embedding.from_pretrained(text_field.vocab.vectors)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs=5\n",
    "#Run the training loop for defined number of epochs\n",
    "for epoch in range(0, num_epochs):\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        #print(inputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        outputs = network(inputs) # Perform forward pass\n",
    "        loss = loss_fn(outputs, targets) # Compute loss\n",
    "        loss.backward() # Backprop\n",
    "        optimizer.step() # Optimization\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                  (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "        i+=1\n",
    "\n",
    "y_true, y_pred = evaluate_model(Network, test_loader)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
