{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMIyjj8TskT_"
      },
      "source": [
        "<center>\n",
        "<h1><br\\></h1>\n",
        "<h1>INF582: INTRODUCTION TO TEXT MINING AND NLP</h1>\n",
        "</center>\n",
        "<center>\n",
        "<h2>Lab Session 1: TF-IDF</h2>\n",
        "<h4>Lecture: Prof. Michalis Vazirgiannis<br>\n",
        "Lab: Dr Guokan Shang and Hadi Abdine <br> contact: hadi.abdine@polytechnique.edu</h4>\n",
        "<h5>Friday, January 12, 2024</h5>\n",
        "<h4><b>Student Name:</b> </h4>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit to Moodle a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing a your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>January 19, 2024 08:29 AM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>\n",
        "\n",
        "<h3><b>1. Introduction</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "Documents are traditionally represented with the vector space model, also known as the bag-of-words representation [<a href='https://nlp.stanford.edu/IR-book/' >Manning et al.</a>]. With this approach, each document di from the collection D = {d1 . . . dm } of size m is associated with an n-dimensional feature vector, where n is the number of unique terms in the preprocessed collection. The set of unique terms T = {t1 . . . tn } is called the vocabulary.\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF), is a method used to compute the coordinates of a document in the vector space.\n",
        "</p>\n",
        "\n",
        "<h3><b>2. Learning Objective</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this lab, you will learn how to compute the TF-IDF representation and use it in 3 different tasks:\n",
        "<ul>\n",
        "<li>computing the cosine similarity between documents</li>\n",
        "<li>executing a query against a collection of documents using the inverted index,</li>\n",
        "<li>performing supervised document classification.</li>\n",
        "</ul>\n",
        "</p>\n",
        "\n",
        "<h3><b>3. Computing TF-IDF</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "The assumption is that the importance of a word to a document increases when its frequency increases. However, considering the frequency as the only factor to judge the importance of a word would result in giving greater weight to commonly used terms such as stopwords, which could be misleading in many tasks. TF-IDF mitigates this problem by introducing a factor that diminishes the weight of words that occur frequently in other documents in the same collection. The TF-IDF weight computation is based on the product of two separate factors, namely the Term Frequency (TF) and the Inverse Document Frequency (IDF). More specifically:\n",
        "\n",
        "$$ tf\\text{-}idf(t,d,\\mathcal{D}) = tf(t,d) \\times idf(t, \\mathcal{D}) $$\n",
        "\n",
        "There are many ways to determine tf and idf . In our case, $tf (t, d)$ is the number of times term $t$ appears in document $d$, and $idf (t, D) = ln (\\frac{m}{1+df (t)}) + 1$, with $df (t)$ the number of documents in the collection $D$ that contains $t$. We can notice that the weight of a term in a document increases when its frequency increases in this document (first factor), and decreases when the number of documents in the collection containing this term increases (second factor). M ∈ $R^{m×n}$ is the TF-IDF matrix of $D$, where $M_{ij}$ is the TF-IDF weight of the $jth$ word in the $ith$ document.\n",
        "\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU8DZ-kVPuBX"
      },
      "source": [
        "### Importing libraries and downloading the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6fcO2sbrulEJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('webkb-test-stemmed.txt', <http.client.HTTPMessage at 0x2cf747247c0>)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2152573&authkey=AFz5kPjESCHRl3s\", 'webkb-train-stemmed.txt')\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2152576&authkey=ACypGA77xWokzQ8\", 'webkb-test-stemmed.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_9_vD4yCkGv"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: </b><br>\n",
        "The next cell implements a <i>tfidf</i> class. When initialized, a <i>tfidf</i> object takes as input a collection of documents and computes the <i>IDF</i> of words in this collection. You can provide a list of stopwords to be ignored. Two methods are defined in this class: <i>tf</i> that returns the frequency of unique words in a document and transform that returns the <i>TF-IDF</i> matrix of the collection. Fill in the gaps in the different functions of <i>tfidf</i> class.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kaN3aNU_svtv"
      },
      "outputs": [],
      "source": [
        "documents = [\"euler is the father of graph theory\",\n",
        "             \"graph theory studies the properties of graphs\",\n",
        "             \"bioinformatics studies the application of efficient algorithms in the biological field\"]\n",
        "\n",
        "class tfidf(object):\n",
        "    def __init__(self, collection, stop_words=[]):\n",
        "        '''collection is a list of strings'''\n",
        "        self.word2ind = {} # map each word in the collection to a unique index\n",
        "        self.ind2word = {}\n",
        "        self.word2idf = {} # map each word to its idf\n",
        "        self.collection = collection\n",
        "        self.documents = [document.split() for document in collection]\n",
        "\n",
        "        all_words = [] # fill it, list of all words in the collection\n",
        "        for document in self.documents:\n",
        "            all_words.extend(document)\n",
        "        self.unique_words = list(set(all_words)) # fill it, list of unique words in the collection\n",
        "        self.unique_words = [word for word in self.unique_words if word not in stop_words] # remove stopwords\n",
        "        self.count_unique_words = len(self.unique_words)\n",
        "        self.word2ind = dict(zip(self.unique_words,range(self.count_unique_words)))\n",
        "        self.ind2word = {v:k for k,v in self.word2ind.items()}\n",
        "        self.count_documents = len(collection)\n",
        "\n",
        "        # compute the idf of unqiue words in the collection\n",
        "        for word in self.word2ind.keys():\n",
        "            count = 0 # fill it, number of documents that contains word\n",
        "            for document in self.documents:\n",
        "                if word in document:\n",
        "                    count += 1\n",
        "            idf = np.log(self.count_documents / (1 + count)) + 1 # fill it\n",
        "            self.word2idf[word] = idf\n",
        "\n",
        "    def getWordFromInd(self, ind):\n",
        "        return self.ind2word[ind]\n",
        "\n",
        "    def getListWords(self):\n",
        "        return self.unique_words\n",
        "\n",
        "    def tf(self, document):\n",
        "        '''\n",
        "        return the frequency of each unique word in document.\n",
        "        document is a list of strings\n",
        "        '''\n",
        "        word2frequency = {}\n",
        "        for word in document:\n",
        "            word2frequency[word] = word2frequency.get(word, 0) + 1 # increment, creating key if it doesn't already exist\n",
        "        return word2frequency\n",
        "\n",
        "    def transform(self, collection):\n",
        "        documents = [document.split() for document in collection] # tokenize documents in the collection\n",
        "        tfidf_mat = np.zeros((len(collection), self.count_unique_words)) # fill it, intialize tfidf matrix with zeros\n",
        "        # compute tfidf\n",
        "        for ind, document in enumerate(documents):\n",
        "            word2frequency = self.tf(document)\n",
        "            for word in word2frequency.keys():\n",
        "                if word in self.word2ind:\n",
        "                    tfidf_mat[ind, self.word2ind[word]] = self.word2idf[word] * word2frequency[word] # fill it, tfidf\n",
        "        return tfidf_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHf1bwk6GGn0"
      },
      "source": [
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 1 (5 points): </b><br>\n",
        "In the above script we provide a small collection documents. In order to validate your functions, compute manually the TF-IDF of the word <b>’the’</b> in the last document and check if you get the same result as with your code.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Zvu2ISM-uckV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4246358550964382\n"
          ]
        }
      ],
      "source": [
        "#tfidf of 'the' in last document\n",
        "t = tfidf(documents)\n",
        "tfidf_mat = t.transform(documents)\n",
        "index = t.word2ind['the']\n",
        "print(tfidf_mat[-1][index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfnpxmskGtfI"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u2-8Oc0HQcj"
      },
      "source": [
        "<h3><b>4. Cosine Similarity</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this section we will compute the cosine similarity of two documents using the TF-IDF representation. The similarity concept is crucial in search engines as well as in text mining applications.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathrm{similarity}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\|v_1\\| \\times \\|v_2\\|}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Normally, the cosine similarity ranges from -1 to +1. However, in our case all the entries of the TF-IDF features are positive, thus it ranges from 0 to 1. The cosine similarity matrix is a square matrix representing the similarity between all pairs of documents in a collection. In other words, if S is the similarity matrix, then $S_{ij} = similarity(v_i , v_j)$ where $v_i$ is the TF-IDF vector of the ith document and $v_j$ is the TF-IDF vector of the jth document.\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDPJQZcTIVbx"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: </b><br>\n",
        "Complete the two functions $\\texttt{cosine_similarity()}$ and $\\texttt{cosine_similarity_matrix()}$ in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OXKsoLZ3Gq3a"
      },
      "outputs": [],
      "source": [
        "documents = [\"i like that music\",\n",
        "\t         \"this song appeals to me\",\n",
        "\t         \"i like graph theory\",\n",
        "\t         \"euler is the father of graph theory\",\n",
        "           \"graph theory studies the properties of graphs\",\n",
        "           \"the quick brown fox jumps over the lazy dog\",\n",
        "           \"the quick fox jumps over the brown lazy dog\"]\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    '''returns the cosine similarity of 2 vectors'''\n",
        "    v1 = np.array(v1)\n",
        "    v2 = np.array(v2)\n",
        "    numerator = v1.dot(v2) # fill it\n",
        "    denominator = np.linalg.norm(v1) * np.linalg.norm(v2) # fill it\n",
        "    return numerator / denominator\n",
        "\n",
        "def cosine_similarity_matrix(mat):\n",
        "    '''\n",
        "    returns the cosine similarity matrix\n",
        "    the ith row in mat represents the ith vector\n",
        "    '''\n",
        "    similarity_matrix = np.zeros((mat.shape[0], mat.shape[0])) # fill it\n",
        "    for i in range(mat.shape[0]):\n",
        "        for j in range(mat.shape[0]):\n",
        "            similarity_matrix[i][j] = cosine_similarity(mat[i], mat[j]) # fill it\n",
        "    return similarity_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBvKViDZJSLN"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 2 (2 points): </b><br>\n",
        "Run the code of the next cell and examine its output. What do you observe about the similarity matrix? What can we do to speed-up the computation?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "048fxiP_JSTk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.         0.4845028  0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         1.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.4845028  0.         1.         0.28294469 0.28294469 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.28294469 1.         0.39794976 0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.28294469 0.39794976 1.         0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]]\n"
          ]
        }
      ],
      "source": [
        "t = tfidf(documents)\n",
        "tfidf_matrix = t.transform(documents)\n",
        "similarity_matrix = cosine_similarity_matrix(tfidf_matrix)\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOBm-xhaJgoD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "The similarity matrix is symmetric and the diagonal is 1. We can speed-up the computation by computing only the upper triangular part of the matrix and then copy it to the lower triangular part.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXRJaBc2M6Rp"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 3 (3 points): </b><br>\n",
        "Notice the similarity between first two documents and between last two documents and state two drawbacks of the bag-of-words representation.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The similarity between the first and second documents is:  0.0\n",
            "The similarity between the two last documents is:  1.0\n"
          ]
        }
      ],
      "source": [
        "similarity_1_2 = similarity_matrix[0][1]\n",
        "similarity_last = similarity_matrix[-1][-2]\n",
        "\n",
        "print('The similarity between the first and second documents is: ', similarity_1_2)\n",
        "print('The similarity between the two last documents is: ', similarity_last)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It1kapOTM_qD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 3: </b><br>\n",
        "Although the first and second documents deal with the same subject, music, the similarity between them is 0. This is because the bag-of-words representation does not take into account the context in which the words appear. Synonyms, variations in word form or changes in word order are not taken into account, here in the first document with \"music\" and in the second with \"song\", which are treated as different.\n",
        "Also, based on the similarity of the two last document, we can say that as the bags-of-words treats each document like an unordered set of words, disregarding the order and structure of the words. This can lead to a loss of semantic meaning.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe7QybieKBht"
      },
      "source": [
        "<h3><b>5. Inverted Index</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "Typically, search engines execute queries against a huge document collection.\n",
        "To execute a query we can simply calculate a similarity measure between the query and all the documents in the collection.<br>\n",
        "However, doing so is very inefficient. Instead, it is possible to eliminate a lot of documents from the candidate set by using the inverted index.\n",
        "The inverted index consists of a mapping from words to documents.\n",
        "Each unique word in the vocabulary is mapped to a list containing the documents in which the word appears and the position of the word in these documents. Stemming, which is reducing tokens to a root form, could be useful in this task. For example a stemming algorithm would reduce <i>computing</i>, <i>computers</i> and <i>computation</i> to <i>comput</i> and thus identifying them as one unique token instead of three different ones. In our code we consider two dictionaries to perform the mapping. In one of the dictionaries the words are stemmed, and in the other they are not.\n",
        "Instead of ranking all the documents in the collection, the inverted index allows us to rank only the relevant documents.\n",
        "\n",
        "In our code we provide several functions:\n",
        "<ul>\n",
        "<li><i>at_least_one_unigram()</i>, returns documents containing at least one query word.\n",
        "<li> <i>all_unigrams()</i>, returns documents containing all query words.\n",
        "<li> <i>ngrams()</i>, returns documents containing all query words in the same order as in the query.\n",
        "</ul>\n",
        "\n",
        "Then, we test all these functionalities using a small corpus containing a limited number of documents. Finally, we execute a query the naive way (examining all documents) and using the inverted index and we compare the execution time.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "o-fHSDc0JhvQ"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# = = = = = = = = = = = =\n",
        "\n",
        "def clean_tokenize(doc):\n",
        "    words = word_tokenize(doc.lower())\n",
        "    return words\n",
        "\n",
        "def index_one_doc(doc,to_stem):\n",
        "    '''\n",
        "    creates dict (tok,positions) for each tok in the document (as term list)\n",
        "    '''\n",
        "    tokpos = dict()\n",
        "    for t_idx,tok in enumerate(doc):\n",
        "        if to_stem:\n",
        "           tok = stemmer.stem(tok)\n",
        "        if tok in tokpos:\n",
        "            tokpos[tok].append(t_idx)\n",
        "        else:\n",
        "            tokpos[tok] = [t_idx]\n",
        "    return tokpos\n",
        "\n",
        "\n",
        "# = = = = = = = = = = = =\n",
        "\n",
        "docs = ['The quick brown fox jumps over the lazy dog',\n",
        "        'The brown quick fox jumps over the lazy dog',\n",
        "        'Luke is the mechanical and electrical engineer of the new group',\n",
        "        'Instead of buying a new engine, buy a new car',\n",
        "        'An engineer may design car engines of all sorts',\n",
        "        'Engineers use logic, but not necessarily imagination',\n",
        "        'Logic will take you from A to Z, imagination will take you everywhere.',\n",
        "        'Continuous effort, not strength or intelligence, is the key to \\\n",
        "        unlocking our potential. And curiosity.',\n",
        "        'It’s OK to have your eggs in one basket as long as you control what \\\n",
        "        happens to that basket.'\n",
        "        ]\n",
        "\n",
        "cleaned_docs = []\n",
        "for doc in docs:\n",
        "    to_app = clean_tokenize(doc)\n",
        "    cleaned_docs.append(to_app)\n",
        "\n",
        "# = = = = = = = = = = = = = = =\n",
        "\n",
        "index_one_doc(cleaned_docs[3],to_stem=True)\n",
        "\n",
        "index_one_doc(cleaned_docs[3],to_stem=False)\n",
        "\n",
        "'''\n",
        "- queries are not case sensitive\n",
        "- we are indexing punctuation marks\n",
        "- we index stopwords and should keep stopwords in the queries (gives more expressivity)\n",
        "'''\n",
        "\n",
        "inverted_index = dict()\n",
        "inverted_index_stem = dict()\n",
        "\n",
        "for d_idx,doc in enumerate(cleaned_docs):\n",
        "\n",
        "    poslists_s = index_one_doc(doc,to_stem=True) # get positions of each token in the doc\n",
        "    for tok,poslist_s in poslists_s.items():\n",
        "        if tok in inverted_index_stem:\n",
        "            inverted_index_stem[tok][d_idx] = poslist_s # update\n",
        "        else:\n",
        "            inverted_index_stem[tok] = dict()\n",
        "            inverted_index_stem[tok][d_idx] = poslist_s # initialize\n",
        "\n",
        "    poslists = index_one_doc(doc,to_stem=False)\n",
        "    for tok, poslist in poslists.items():\n",
        "        if tok in inverted_index:\n",
        "            inverted_index[tok][d_idx] = poslist\n",
        "        else:\n",
        "            inverted_index[tok] = dict()\n",
        "            inverted_index[tok][d_idx] = poslist\n",
        "\n",
        "\n",
        "# = = = = = = = = = = = = = = = = =\n",
        "\n",
        "def at_least_one_unigram(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing *at least one* query unigrams\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "\n",
        "    to_return = []\n",
        "    for unigram in query:\n",
        "        if unigram in inverted_index:\n",
        "            to_return.extend(list(inverted_index[unigram].keys()))\n",
        "    return list(set(to_return))\n",
        "\n",
        "def all_unigrams(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing *all* query unigrams\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "\n",
        "    to_return = []\n",
        "    for unigram in query:\n",
        "        if unigram in inverted_index:\n",
        "            to_return.append(set(list(inverted_index[unigram].keys())))\n",
        "        else:\n",
        "            to_return.append(set())\n",
        "            break\n",
        "    to_return = to_return[0].intersection(*to_return)\n",
        "    return list(to_return)\n",
        "\n",
        "def ngrams(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing all unigrams in same order as the query\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "    candidate_docs = all_unigrams(query,inverted_index)\n",
        "\n",
        "    to_return = []\n",
        "    for doc in candidate_docs:\n",
        "        poslists = []\n",
        "        for unigram in query:\n",
        "            to_append = inverted_index[unigram][doc]\n",
        "            if isinstance(to_append, int):\n",
        "                poslists.append([to_append])\n",
        "            else:\n",
        "                poslists.append(to_append)\n",
        "        # test whether the query words are consecutive\n",
        "        poslists_sub = [[elt-idx for elt in poslist] for idx,poslist in enumerate(poslists)]\n",
        "        if set(poslists_sub[0]).intersection(*poslists_sub):\n",
        "            to_return.append(doc)\n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSfqenBLte7"
      },
      "source": [
        "## Queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5acI7y3yVqnH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: engine car\n",
            "-----------------------------------------------\n",
            "docs containing at least one word in the query:\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "\n",
            "docs containing all the words in the query:\n",
            "* Instead of buying a new engine, buy a new car\n",
            "\n",
            "docs (stemmed) containing at least one word in the query (stemmed):\n",
            "* Luke is the mechanical and electrical engineer of the new group\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "* Engineers use logic, but not necessarily imagination\n",
            "\n",
            "docs (stemmed) containing all the words in the query (stemmed):\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "\n",
            "docs containing engine all the words in the query in the same order:\n",
            "********************************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = ['engine','car']\n",
        "print('Query: {}'.format(' '.join(query)))\n",
        "print('-----------------------------------------------')\n",
        "\n",
        "docs_index = at_least_one_unigram(query,inverted_index)\n",
        "print('docs containing at least one word in the query:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = all_unigrams(query,inverted_index)\n",
        "print('docs containing all the words in the query:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "query_stemmed = [stemmer.stem(elt) for elt in query]\n",
        "docs_index = at_least_one_unigram(query_stemmed,inverted_index_stem)\n",
        "print('docs (stemmed) containing at least one word in the query (stemmed):')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = all_unigrams(query_stemmed,inverted_index_stem)\n",
        "print('docs (stemmed) containing all the words in the query (stemmed):')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = ngrams(query,inverted_index)\n",
        "print('docs containing engine all the words in the query in the same order:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print('********************************************************')\n",
        "print()\n",
        "\n",
        "tf_idf = tfidf(docs)\n",
        "query = ['new', 'car']\n",
        "tf_idf_query = tf_idf.transform([(' ').join(query)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvlLzsRtLRla"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "Complete the code in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Fqx4g0guLvED"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query (new car) over all documents\n",
            "-----------------------------------------------\n",
            "Top similar document: Instead of buying a new engine, buy a new car\n",
            "Found in 0.0 ms\n",
            "\n",
            "Query (new car) over candidates containing at least one of the words\n",
            "-----------------------------------------------\n",
            "Top similar document: Instead of buying a new engine, buy a new car\n",
            "Found in 0.0 ms\n"
          ]
        }
      ],
      "source": [
        "############ query over all documents\n",
        "print('Query ({}) over all documents'.format((' ').join(query)))\n",
        "print('-----------------------------------------------')\n",
        "time1 = time.time()\n",
        "tf_idf_collection = tf_idf.transform(docs) # fill it\n",
        "size = tf_idf_collection.shape[0]\n",
        "scores = [cosine_similarity(tf_idf_query, tf_idf_collection[i]) for i in range(size)] # fill it, list containing the cosine similarities of the query against all documents\n",
        "\n",
        "time2 = time.time()\n",
        "print(\"Top similar document: {}\".format(docs[np.argmax(scores)]))\n",
        "print(\"Found in {} ms\".format((time2 - time1)*1000))\n",
        "\n",
        "print('')\n",
        "\n",
        "############ query over candidate documents using inverted index\n",
        "print('Query ({}) over candidates containing at least one of the words'.format((' ').join(query)))\n",
        "print('-----------------------------------------------')\n",
        "time1 = time.time()\n",
        "candidate_docs_index = at_least_one_unigram(query,inverted_index) # fill it\n",
        "candidate_docs = [docs[el] for el in candidate_docs_index]\n",
        "tf_idf_collection = tf_idf.transform(candidate_docs)\n",
        "size = tf_idf_collection.shape[0]\n",
        "scores = [cosine_similarity(tf_idf_query, tf_idf_collection[i]) for i in range(size)] # fill it, list containing the cosine similarities of the query against candidate documents\n",
        "time2 = time.time()\n",
        "print(\"Top similar document: {}\".format(candidate_docs[np.argmax(scores)]))\n",
        "print(\"Found in {} ms\".format((time2 - time1)*1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z8vwmdeM0jv"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 4 (2.5 points): </b><br>\n",
        "Examine and interpret the output of the previous cell.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYhAq72iNMAc"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "As result we get the same document if we look into all the documents or the documents containing at least one query word, and this is logical because the score of a document that doesn't contains any word of the query is small that the score of a document that contains at least one word of the query as the TF-IDF is computed based on the frequency of the words in the document. So, looking into all the documents or the documents containing at least one query word gives the same result.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORYNxB_yNMH9"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 5 (2.5 points): </b><br>\n",
        "If we execute a query the naive way, and then we execute it against the documents containing all the words in the query, is it possible to get different results? Motivate your answer\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9huKEjZ6NMXr"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 5: </b><br>\n",
        "Yes, it is possible to get a different result because if the list of documents containing all the words in the query is empty, the result will be empty, while the naive way will possibly return some documents since some documents may contain some of the words in the query. \n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R959CcOlNmfn"
      },
      "source": [
        "<h3><b>6. Supervised Classification</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this section we will use the TF-IDF features to perform a supervised classification task. We will work with the WebKB dataset. It features academic webpages belonging to four different categories: (1) project, (2) course, (3) faculty, and (4) students, and contains 2,803 documents for training and 1,396 for testing. Documents have already been preprocessed with stopword removal and Porter stemming. The code that you will work with implements the following steps:\n",
        "\n",
        "<ul>\n",
        "<li>data loading,\n",
        "<li>computation of TF-IDF features for the training set,\n",
        "<li> computation of features for the test set. Note that the documents in the test set are represented in the space made of the unique terms in the training set only (words in the testing set absent from the training set are disregarded).\n",
        "<li>classifier training/testing. Naive Bayes classifier [<a href='https://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf'>McCallum and Nigam, 1998</a>], Multinomial Logistic Regression [<a href='https://www.learningtheory.org/colt2000/papers/CollinsSchapireSinger.pdf'>Collins et al., 2002</a>], Ran- dom Forest Classifier [1] and linear kernel SVM [<a href='https://link.springer.com/article/10.1007/BF00994018'>Cortes and Vapnik 1995</a>, <a href='https://www.researchgate.net/publication/28351286_Text_Categorization_with_Support_Vector_Machines'>Joachims 1998</a>] are compared,\n",
        "<li>get the most/least important words per class.\n",
        "</ul>\n",
        "\n",
        "Then, we test all these functionalities using a small corpus containing a limited number of documents. Finally, we execute a query the naive way (examining all documents) and using the inverted index and we compare the execution time.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjbbZSmPVpu"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Complete the code in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QaDtszVDNLg8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logisitc Regression accuracy: 0.8810888252148997\n",
            "Random forest accuracy: 0.8481375358166189\n",
            "Naive Bayes accuracy: 0.8173352435530086\n",
            "Linear SVM accuracy: 0.8474212034383954\n",
            "0.2679083094555874\n",
            "\n",
            "Top 10\n",
            "student: address wisc graduat interest home zhang advisor work construct resum\n",
            "course: hui grade structur materi data assign instructor comp syllabu fall\n",
            "project: softwar faculti perform project peopl high lab hybrid request group\n",
            "faculty: ufl cours perman recent cpsc associ teach henri fax professor\n",
            "Bottom 10\n",
            "student: professor henri perman faculti group fall hybrid comp fax\n",
            "course: research cours interest vision berkelei cpsc person pictur graphic\n",
            "project: interest home address fall email offic fax professor scienc\n",
            "faculty: graduat lab student construct advisor homework syllabu move resum\n"
          ]
        }
      ],
      "source": [
        "def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    # coef stores the weights of each feature (in unique term), for each class\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"%s: %s\" % (class_label,\" \".join(feature_names[j] for j in top10)))\n",
        "\n",
        "def print_bot10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the lowest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        bot10 = np.argsort(clf.coef_[i])[0:9]\n",
        "        print(\"%s: %s\" % (class_label,\" \".join(feature_names[j] for j in bot10)))\n",
        "\n",
        "def prepare_data(path):\n",
        "    with open(path, 'r') as f:\n",
        "        s = f.read()\n",
        "    examples = s.split('\\n') # split to samples\n",
        "    examples = examples[:-1] # last element is empty\n",
        "    documents = []\n",
        "    labels = []\n",
        "    for el in examples:\n",
        "        example = el.split('\\t') # separate document from label\n",
        "        documents.append(example[1])\n",
        "        labels.append(example[0])\n",
        "    return documents, labels\n",
        "\n",
        "path_train = './webkb-train-stemmed.txt' # path to train data\n",
        "path_test = './webkb-test-stemmed.txt' # path to test data\n",
        "train_documents, train_labels = prepare_data(path_train)\n",
        "test_documents, test_labels = prepare_data(path_test)\n",
        "\n",
        "categories = set(train_labels) # get unique categoris\n",
        "category2ind = dict(zip(categories,range(len(categories)))) # map each category to index\n",
        "ind2category = {v:k for k,v in category2ind.items()} # map index to category\n",
        "\n",
        "train_labels_index = [category2ind[cat] for cat in train_labels] # replace labels by their indexes\n",
        "test_labels_index = [category2ind[cat] for cat in test_labels] # replace labels by their indexes\n",
        "\n",
        "t = tfidf(train_documents) # fill it, tfidf object\n",
        "tfidf_train = t.transform(train_documents) # fill it, get the tfidf training matrix\n",
        "tfidf_test = t.transform(test_documents) # fill it, get the tfidf text matrix\n",
        "\n",
        "lr = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=200)\n",
        "rf = RandomForestClassifier(n_estimators=20)\n",
        "nb = MultinomialNB()\n",
        "svm = LinearSVC(max_iter=2000)\n",
        "\n",
        "Classifiers = {'Logisitc Regression': lr, 'Random forest': rf, 'Naive Bayes': nb, 'Linear SVM': svm}\n",
        "\n",
        "for classifier in Classifiers.keys():\n",
        "    Classifiers[classifier].fit(tfidf_train, train_labels_index) # train each classifier\n",
        "    predicted = Classifiers[classifier].predict(tfidf_test) # perform prediction\n",
        "    accuracy = 0 # fill it, compute accuracy\n",
        "    for i in range(len(predicted)):\n",
        "        if predicted[i] == test_labels_index[i]:\n",
        "            accuracy += 1\n",
        "    accuracy /= len(predicted)\n",
        "    print('{} accuracy: {}'.format(classifier, accuracy))\n",
        "\n",
        "predicted = np.zeros((len(test_labels_index))) + 3\n",
        "print(np.mean(predicted == np.array(test_labels_index)))\n",
        "print('')\n",
        "\n",
        "# choose one classfier and print top 10 and bottom 10 words per class\n",
        "feature_names = t.ind2word  # fill it\n",
        "classifier = lr # fill it\n",
        "print('Top 10')\n",
        "print_top10(feature_names, classifier, categories)\n",
        "print('Bottom 10')\n",
        "print_bot10(feature_names, classifier, categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IjXDedgQFxn"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 6 (5 points): </b><br>\n",
        "Examine the accuracy of different classifiers, examine the most/least important words and comment the results.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKQ-IEKTQUzD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 6: </b><br>\n",
        "The accuracy of the different classifiers is pretty similar around 0.80 even if the Naive Bayes classifier is the least accurate. The most accurate is the Logistic Regression classifier, the random forest classifier and the Linear SVM are in the middle.\n",
        "\n",
        "The most important words seem to be related to general academic or professional topics, such as \"address,\" \"research,\" \"project,\" and \"faculty.\" These words are likely indicative of the content or context of the data being classified.\n",
        "The least important words appear to be a mix of specific terms and possibly less informative words, such as \"professor,\" \"fall,\" \"hybrid,\" and \"fax.\" It's important to note that without context, it's challenging to interpret the significance of these words accurately.\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
